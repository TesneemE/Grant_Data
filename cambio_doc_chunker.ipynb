{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12536605,"sourceType":"datasetVersion","datasetId":7914470}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport re\nfrom datetime import datetime\nfrom pathlib import Path\nfrom shutil import copyfile\nfrom urllib.parse import quote_plus\n\nimport nltk\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\nfrom kaggle_secrets import UserSecretsClient\n!pip install --quiet langchain langchain-community unstructured[docx]\n\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import pipeline\n# from langchain_unstructured import UnstructuredLoader as UnstructuredFileLoader\nfrom langchain_community.document_loaders import UnstructuredFileLoader\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass, asdict\nimport logging\n\ntry:\n    from docx import Document\n    DOCX_AVAILABLE = True\nexcept ImportError:\n    DOCX_AVAILABLE = False\n    print(\"python-docx not available. Install with: pip install python-docx\")\n\ntry:\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n    SKLEARN_AVAILABLE = True\nexcept ImportError:\n    SKLEARN_AVAILABLE = False\n    print(\"scikit-learn not available. Install with: pip install scikit-learn\")\n\ntry:\n    import spacy\n    # Try to load English model\n    try:\n        nlp = spacy.load(\"en_core_web_sm\")\n        SPACY_AVAILABLE = True\n    except OSError:\n        SPACY_AVAILABLE = False\n        print(\"spaCy English model not available. Install with: python -m spacy download en_core_web_sm\")\nexcept ImportError:\n    SPACY_AVAILABLE = False\n    print(\"spaCy not available. Install with: pip install spacy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:08:30.619829Z","iopub.execute_input":"2025-07-24T15:08:30.620427Z","iopub.status.idle":"2025-07-24T15:09:32.995409Z","shell.execute_reply.started":"2025-07-24T15:08:30.620393Z","shell.execute_reply":"2025-07-24T15:09:32.994790Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-07-24 15:09:09.079828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753369749.449540      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753369749.555701      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# # === NLTK Setup ===\n# required_nltk_packages = [\n#     \"punkt\", \"averaged_perceptron_tagger\", \"maxent_ne_chunker\", \"words\", \"stopwords\"\n# ]\n\n# def nltk_package_path(package):\n#     return {\n#         \"punkt\": \"tokenizers/punkt\",\n#         \"averaged_perceptron_tagger\": \"taggers/averaged_perceptron_tagger\",\n#         \"maxent_ne_chunker\": \"chunkers/maxent_ne_chunker\",\n#         \"words\": \"corpora/words\",\n#         \"stopwords\": \"corpora/stopwords\"\n#     }.get(package, package)\n\n# for package in required_nltk_packages:\n#     try:\n#         nltk.data.find(f\"{nltk_package_path(package)}\")\n#     except LookupError:\n#         nltk.download(package)\n\n# # === Models ===\n# embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n# llm = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\", device=0, max_new_tokens=512)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:09:32.996974Z","iopub.execute_input":"2025-07-24T15:09:32.997703Z","iopub.status.idle":"2025-07-24T15:09:33.001423Z","shell.execute_reply.started":"2025-07-24T15:09:32.997680Z","shell.execute_reply":"2025-07-24T15:09:33.000894Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# === Config ===\n# WATCH_FOLDER_ID = \"17sSWn0cfX-jEFmgG4Hl-P9_NtG9kqS6D\"\nWATCH_FOLDER_ID=\"1tJhw1KbempeSjoHfkJ-Ig6Ikn-bKL20Q\" #test folder\nDOWNLOAD_PATH = Path(\"/tmp/gdrive_docs\")\nDOWNLOAD_PATH.mkdir(parents=True, exist_ok=True)\n\n# === Auth ===\nCRED_INPUT = \"/kaggle/input/google-cred/credentials.json\"\nCRED_WORKING = \"/kaggle/working/credentials.json\"\ncopyfile(CRED_INPUT, CRED_WORKING)\n\ngauth = GoogleAuth()\ngauth.LoadCredentialsFile(CRED_WORKING)\nif gauth.credentials is None:\n    gauth.LocalWebserverAuth()\nelif gauth.access_token_expired:\n    gauth.Refresh()\nelse:\n    gauth.Authorize()\ngauth.SaveCredentialsFile(CRED_WORKING)\ndrive = GoogleDrive(gauth)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:09:33.002203Z","iopub.execute_input":"2025-07-24T15:09:33.002470Z","iopub.status.idle":"2025-07-24T15:09:33.146544Z","shell.execute_reply.started":"2025-07-24T15:09:33.002446Z","shell.execute_reply":"2025-07-24T15:09:33.145947Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# === HELPERS ===\ndef sanitize_filename(title):\n    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", title)\n\ndef detect_format_style(text):\n    bold_qs = len(re.findall(r\"\\n[A-Z][^\\n]{4,100}\\?\\s*\\n\", text))\n    return \"bolded_qs\" if bold_qs > 3 else \"narrative\"\n\ndef llm_extract_qa_pairs(text):\n    prompt = (\n        \"\"\"Extract question and answer pairs from the grant text below. Use format:\nQ: [Question]\nA: [Answer]\n\nText:\n\"\"\"\n        f\"{text}\"\n    )\n    try:\n        output = llm(prompt.strip())[0][\"generated_text\"]\n        pairs = re.findall(r\"Q:\\s*(.+?)\\s*A:\\s*(.+?)(?=\\nQ:|\\Z)\", output, re.DOTALL)\n        return [{\"question\": q.strip(), \"answer\": a.strip()} for q, a in pairs]\n    except Exception as e:\n        print(f\"âš ï¸ LLM Q&A extraction failed: {e}\")\n        return []\n\ndef extract_qa_pairs(text):\n    q_pattern = re.compile(r\"(?=(?:^|\\n)([^:\\n]{4,100}[\\?:])\\s*\\n?)\", re.MULTILINE)\n    splits = q_pattern.split(text)\n    qa_pairs = []\n    for i in range(1, len(splits), 2):\n        question = splits[i].strip()\n        answer = splits[i+1].strip() if i+1 < len(splits) else \"\"\n        if len(answer) > 10 and len(question) > 5:\n            qa_pairs.append({\"question\": question, \"answer\": answer})\n    return qa_pairs\n\ndef categorize_chunk(text: str, question: str = \"\") -> list:\n    categories = []\n    lower_text = text.lower()\n    lower_q = question.lower()\n    if \"problem\" in lower_q or \"we address\" in lower_text:\n        categories.append(\"Contact Information Problem\")\n    if \"mission\" in lower_q or \"mission\" in lower_text:\n        categories.append(\"Mission Statement\")\n    if len(text) < 400:\n        categories.append(\"Project Summary\")\n    if \"goal\" in lower_q or \"vision\" in lower_q:\n        categories.append(\"Goals, Vision, or Objectives\")\n    if \"approach\" in lower_q or \"methodology\" in lower_text:\n        categories.append(\"Our Solution or Approach\")\n    if \"impact\" in lower_q or \"results\" in lower_q:\n        categories.append(\"Impact Results or Outcomes\")\n    if \"who benefits\" in lower_q or \"target population\" in lower_q:\n        categories.append(\"Beneficiaries\")\n    if \"unique\" in lower_text:\n        categories.append(\"Unique Value Proposition\")\n    if \"timeline\" in lower_q:\n        categories.append(\"Plan and Timeline\")\n    if \"budget\" in lower_q or \"$\" in text:\n        categories.append(\"Budget and Funding\")\n    if \"sustainability\" in lower_text:\n        categories.append(\"Sustainability or Strategy\")\n    if \"team\" in lower_q or \"lived experience\" in lower_text:\n        categories.append(\"Team Members and Descriptions\")\n    if re.search(r\"\\\\bfounded\\\\b|\\\\bhistory\\\\b|\\\\baccelerator\\\\b\", lower_text):\n        categories.append(\"Organizational History\")\n    if re.search(r\"(https?://\\\\S+)\", lower_text):\n        categories.append(\"Supplementary Materials\")\n    if not categories:\n        categories.append(\"Miscellaneous\")\n    return categories\n\ndef annotate_chunk(chunk_text: str) -> dict:\n    prompt = (\n        \"\"\"You are a grant document assistant.\nAnalyze the following text and return the most likely category it fits into.\nPossible categories include: Mission Statement, Problem, Team, Impact, Sustainability, etc.\nReturn only JSON in the format:\n{\"question\": \"...\", \"answer\": \"...\", \"category\": \"...\"}\n\nTEXT:\n\"\"\"\n        f\"{chunk_text}\"\n    )\n    try:\n        result = llm(prompt.strip())[0]['generated_text']\n        match = re.search(r\"\\{.*\\}\", result, re.DOTALL)\n        if match:\n            return json.loads(match.group())\n        else:\n            raise ValueError(\"No JSON found in model output\")\n    except Exception as e:\n        print(f\"âš ï¸ LLM error: {e}\")\n        return {\"question\": None, \"answer\": None, \"category\": \"Miscellaneous\"}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:09:33.147188Z","iopub.execute_input":"2025-07-24T15:09:33.147354Z","iopub.status.idle":"2025-07-24T15:09:33.158453Z","shell.execute_reply.started":"2025-07-24T15:09:33.147337Z","shell.execute_reply":"2025-07-24T15:09:33.157766Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# === Doc Chunker ===\n\n@dataclass\nclass DocumentChunk:\n    chunk_id: int\n    header: str\n    questions: List[str]\n    content: str\n    topics: List[str]\n    word_count: int\n    confidence_score: float = 0.0\n\nclass DocumentChunker:\n    def __init__(self):\n        self.setup_logging()\n        \n        # Patterns for different document types\n        self.patterns = {\n            'grant_application': {\n                'header_patterns': [\n                    r'\\*\\*([^*]+)\\*\\*',  # **Header**\n                    r'^([A-Z][^a-z]*[A-Z])$',  # ALL CAPS\n                    r'^([A-Z][A-Za-z\\s]+)$',  # Title Case\n                ],\n                'question_patterns': [\n                    r'^.+\\?$',  # Ends with question mark\n                    r'^\\*?Please .+',  # Starts with \"Please\"\n                    r'^How .+',  # Starts with \"How\"\n                    r'^What .+',  # Starts with \"What\"\n                    r'^Describe .+',  # Starts with \"Describe\"\n                ],\n                'section_markers': [\n                'project summary', 'contact information', 'mission statement', 'mission','fit', 'alignment', 'grant', 'goals', 'vision', 'objectives', 'objective','solution', 'approach', 'impact', 'results', 'outcomes', 'outcome','beneficiaries', 'beneficiary', 'unique value proposition', 'value proposition','plan', 'timeline', 'budget', 'funding', 'sustainability', 'strategy','team members', 'team', 'descriptions', 'supplementary materials','organizational history', 'history', 'background', 'program', 'description','partnership', 'miscellaneous']\n            }\n        }\n        \n        # Topic keywords for classification\n        self.topic_keywords = {\n            'education': ['education', 'learning', 'student', 'curriculum', 'teaching', 'school'],\n            'entrepreneurship': ['entrepreneur', 'startup', 'business', 'venture', 'innovation'],\n            'solar': ['solar', 'renewable', 'energy', 'installation', 'panel', 'clean energy'],\n            'workforce': ['job', 'employment', 'training', 'skills', 'workforce', 'career'],\n            'community': ['community', 'neighborhood', 'local', 'resident', 'housing'],\n            'BIPOC': ['BIPOC', 'youth', 'color', 'underestimated', 'marginalized'],\n            'technology': ['technology', 'platform', 'digital', 'AI', 'tech'],\n            'partnership': ['partner', 'collaboration', 'alliance', 'cooperation'],\n            'funding': ['funding', 'grant', 'budget', 'revenue', 'cost', 'financial'],\n            'metrics': ['metric', 'outcome', 'result', 'measure', 'data', 'statistics']\n        }\n\n    def setup_logging(self):\n        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n        self.logger = logging.getLogger(__name__)\n\n    def extract_text_from_docx(self, file_path: str) -> str:\n        \"\"\"Extract text from DOCX file preserving some formatting\"\"\"\n        if not DOCX_AVAILABLE:\n            raise ImportError(\"python-docx required for DOCX files\")\n            \n        doc = Document(file_path)\n        text_parts = []\n        \n        for paragraph in doc.paragraphs:\n            # Check if paragraph is bold (header-like)\n            if paragraph.runs and any(run.bold for run in paragraph.runs):\n                text_parts.append(f\"**{paragraph.text}**\")\n            else:\n                text_parts.append(paragraph.text)\n                \n        return '\\n'.join(text_parts)\n\n    def detect_document_type(self, text: str) -> str:\n        \"\"\"Detect document type based on content patterns\"\"\"\n        text_lower = text.lower()\n        \n        # Check for grant application indicators\n        grant_indicators = ['grant', 'funding', 'organization', 'mission', 'program description']\n        grant_score = sum(1 for indicator in grant_indicators if indicator in text_lower)\n        \n        if grant_score >= 3:\n            return 'grant_application'\n        \n        return 'generic'\n\n    def extract_headers(self, text: str, doc_type: str = 'generic') -> List[Dict]:\n        \"\"\"Extract headers using multiple pattern matching approaches\"\"\"\n        patterns = self.patterns.get(doc_type, self.patterns['grant_application'])\n        headers = []\n        \n        lines = text.split('\\n')\n        \n        for i, line in enumerate(lines):\n            line = line.strip()\n            if not line:\n                continue\n                \n            # Try each header pattern\n            for pattern in patterns['header_patterns']:\n                match = re.match(pattern, line)\n                if match:\n                    header_text = match.group(1) if match.groups() else line\n                    headers.append({\n                        'text': header_text.strip('*').strip(),\n                        'line_number': i,\n                        'pattern_type': 'header'\n                    })\n                    break\n            \n            # Check for question patterns\n            for pattern in patterns['question_patterns']:\n                if re.match(pattern, line, re.IGNORECASE):\n                    headers.append({\n                        'text': line,\n                        'line_number': i,\n                        'pattern_type': 'question'\n                    })\n                    break\n\n        return headers\n\n    def chunk_by_headers(self, text: str, headers: List[Dict]) -> List[Dict]:\n        \"\"\"Split text into chunks based on detected headers\"\"\"\n        lines = text.split('\\n')\n        chunks = []\n        \n        for i, header in enumerate(headers):\n            start_line = header['line_number']\n            end_line = headers[i + 1]['line_number'] if i + 1 < len(headers) else len(lines)\n            \n            # Extract content between headers\n            content_lines = lines[start_line + 1:end_line]\n            content = '\\n'.join(content_lines).strip()\n            \n            # Separate questions from content\n            questions = []\n            content_parts = []\n            \n            for line in content_lines:\n                line = line.strip()\n                if line.endswith('?') and len(line.split()) <= 20:  # Likely a question\n                    questions.append(line)\n                elif line:\n                    content_parts.append(line)\n            \n            if content or questions:\n                chunk = {\n                    'header': header['text'],\n                    'questions': questions,\n                    'content': '\\n'.join(content_parts).strip(),\n                    'start_line': start_line,\n                    'end_line': end_line,\n                    'pattern_type': header['pattern_type']\n                }\n                chunks.append(chunk)\n        \n        return chunks\n\n    def extract_topics_tfidf(self, text: str, max_features: int = 3) -> List[str]:\n        \"\"\"Extract top 1-3 topics using TF-IDF\"\"\"\n        if not SKLEARN_AVAILABLE:\n            return self.extract_topics_keyword_matching(text)\n        \n        try:\n            # Clean text\n            text_clean = re.sub(r'[^\\w\\s]', ' ', text.lower())\n            text_clean = re.sub(r'\\s+', ' ', text_clean).strip()\n            \n            if len(text_clean.split()) < 10:  # Too short for TF-IDF\n                return self.extract_topics_keyword_matching(text)\n            \n            vectorizer = TfidfVectorizer(\n                max_features=max_features*2, #gets more candidates to filter\n                stop_words='english',\n                ngram_range=(1, 2),\n                min_df=1,\n                max_df=0.8\n            )\n            \n            tfidf_matrix = vectorizer.fit_transform([text_clean])\n            feature_names = vectorizer.get_feature_names_out()\n            scores = tfidf_matrix.toarray()[0]\n            \n            # Get top terms\n            topic_scores = list(zip(feature_names, scores))\n            topic_scores.sort(key=lambda x: x[1], reverse=True)\n            \n            topics = [term for term, score in topic_scores if score > 0][:max_features]\n            \n            # Enhance with keyword matching if fewer than max\n#            keyword_topics = self.extract_topics_keyword_matching(text)\n            if len(topics) < max_features:\n                keyword_topics = self.extract_topics_keyword_matching(text)\n            # Combine and deduplicate\n#            all_topics = list(set(topics + keyword_topics))\n            for topic in keyword_topics:\n                if topic not in topics and len(topics)<max_features:\n                    topics.append(topic)\n            return topics[:max_features]\n            \n        except Exception as e:\n            self.logger.warning(f\"TF-IDF topic extraction failed: {e}\")\n            return self.extract_topics_keyword_matching(text)\n\n    def extract_topics_keyword_matching(self, text: str, max_topics: int = 3) -> List[str]:\n        \"\"\"Extract top 1-3 topics using keyword matching\"\"\"\n        text_lower = text.lower()\n        found_topics = []\n        specific_terms = []\n        topic_priority = [\n        'education', 'entrepreneurship', 'solar', 'workforce', 'BIPOC',\n        'community', 'technology', 'partnership', 'funding', 'metrics'\n        ]\n        for topic in topic_priority:\n            if len(found_topics)>=max_topics:\n                break\n            keywords=self.topic_keywords[topic]\n            if any(keyword in text_lower for keyword in keywords):\n                found_topics.append(topic)\n#        for topic, keywords in self.topic_keywords.items():\n#            if any(keyword in text_lower for keyword in keywords):\n#                found_topics.append(topic)\n#        \n        # Add specific terms found in text\n        if len(found_topics)<max_topics:\n            for word in text_lower.split():\n                word = re.sub(r'[^\\w]', '', word)\n                if (len(word) > 3 and\n                word not in ENGLISH_STOP_WORDS if SKLEARN_AVAILABLE else True and\n                word.isalpha()):\n                    specific_terms.append(word)\n        \n        # Get most frequent specific terms\n        from collections import Counter\n        term_counts = Counter(specific_terms)\n        common_terms = [term for term, count in term_counts.most_common(3) if count > 1]\n        for term in common_terms:\n            if len(found_topics)>=max_topics:\n                break\n            if term not in found_topics:\n                found_topics.append(term)\n        return found_topics[:max_topics]\n\n    def extract_topics_spacy(self, text: str) -> List[str]:\n        \"\"\"Extract topics using spaCy NER and noun phrases\"\"\"\n        if not SPACY_AVAILABLE:\n            return self.extract_topics_keyword_matching(text, max_topics=3)\n        \n        try:\n            doc = nlp(text[:1000])  # Limit text length for performance\n            \n            # Extract named entities\n            entities = [ent.text.lower() for ent in doc.ents \n                       if ent.label_ in ['ORG', 'PRODUCT', 'EVENT', 'WORK_OF_ART']]\n            \n            # Extract noun phrases\n            noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks \n                           if len(chunk.text.split()) <= 3]\n            \n            # Combine with keyword matching\n            keyword_topics = self.extract_topics_keyword_matching(text, max_topics=3)\n            \n            all_topics = list(set(entities + noun_phrases + keyword_topics))\n            return [topic for topic in all_topics if len(topic) > 2][:3]\n            \n        except Exception as e:\n            self.logger.warning(f\"spaCy topic extraction failed: {e}\")\n            return self.extract_topics_keyword_matching(text, max_topics=3)\n\n    def calculate_confidence_score(self, chunk: Dict) -> float:\n        \"\"\"Calculate confidence score for chunk quality\"\"\"\n        score = 0.0\n        \n        # Header quality\n        if chunk['header']:\n            score += 0.3\n            if len(chunk['header'].split()) > 1:\n                score += 0.1\n        \n        # Content quality\n        if chunk['content']:\n            word_count = len(chunk['content'].split())\n            if word_count > 20:\n                score += 0.3\n            if word_count > 100:\n                score += 0.1\n        \n        # Questions present\n        if chunk['questions']:\n            score += 0.2\n        \n        # Topic relevance\n        if len(chunk.get('topics', [])) >= 1:\n            score += 0.1\n            if len(chunk.get('topics',[]))>=2:\n                score+=.05\n        \n        return min(score, 1.0)\n\n    def process_document(self, file_path: str, output_path: Optional[str] = None) -> List[DocumentChunk]:\n        \"\"\"Main method to process a document and extract chunks\"\"\"\n        self.logger.info(f\"Processing document: {file_path}\")\n        \n        # Read file\n        file_path = Path(file_path)\n        if file_path.suffix.lower() == '.docx':\n            text = self.extract_text_from_docx(str(file_path))\n        else:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                text = f.read()\n        \n        # Detect document type\n        doc_type = self.detect_document_type(text)\n        self.logger.info(f\"Detected document type: {doc_type}\")\n        \n        # Extract headers\n        headers = self.extract_headers(text, doc_type)\n        self.logger.info(f\"Found {len(headers)} headers/sections\")\n        \n        # Create chunks\n        raw_chunks = self.chunk_by_headers(text, headers)\n        \n        # Process chunks\n        processed_chunks = []\n        for i, chunk_data in enumerate(raw_chunks):\n            # Extract topics\n            full_text = f\"{chunk_data['header']} {' '.join(chunk_data['questions'])} {chunk_data['content']}\"\n            \n            # Try multiple topic extraction methods\n            topics = self.extract_topics_tfidf(full_text, max_features=3)\n            if not topics:\n                topics = self.extract_topics_keyword_matching(full_text, max_topics=3)\n            \n            # Create DocumentChunk\n            chunk = DocumentChunk(\n                chunk_id=i + 1,\n                header=chunk_data['header'],\n                questions=chunk_data['questions'],\n                content=chunk_data['content'],\n                topics=topics,\n                word_count=len(chunk_data['content'].split()) if chunk_data['content'] else 0,\n                confidence_score=self.calculate_confidence_score(chunk_data)\n            )\n            \n            processed_chunks.append(chunk)\n        \n        # Save to JSON if output path provided\n        if output_path:\n            self.save_chunks_to_json(processed_chunks, output_path, file_path.name)\n        \n        return processed_chunks\n\n    def save_chunks_to_json(self, chunks: List[DocumentChunk], output_path: str, document_name: str):\n        \"\"\"Save chunks to JSON file\"\"\"\n        output_data = {\n            \"document_title\": document_name,\n            \"total_chunks\": len(chunks),\n            \"chunks\": [asdict(chunk) for chunk in chunks]\n        }\n        \n        with open(output_path, 'w', encoding='utf-8') as f:\n            json.dump(output_data, f, indent=2, ensure_ascii=False)\n        \n        self.logger.info(f\"Saved {len(chunks)} chunks to {output_path}\")\n\n    def batch_process(self, input_dir: str, output_dir: str):\n        \"\"\"Process multiple documents in a directory\"\"\"\n        input_path = Path(input_dir)\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n        \n        supported_extensions = ['.txt', '.docx']\n        files = [f for f in input_path.glob('*') if f.suffix.lower() in supported_extensions]\n        \n        self.logger.info(f\"Found {len(files)} files to process\")\n        \n        for file_path in files:\n            try:\n                output_file = output_path / f\"{file_path.stem}_chunks.json\"\n                chunks = self.process_document(str(file_path), str(output_file))\n                self.logger.info(f\"Successfully processed {file_path.name}: {len(chunks)} chunks\")\n            except Exception as e:\n                self.logger.error(f\"Failed to process {file_path.name}: {e}\")\n\n# def main():\n#     \"\"\"Example usage\"\"\"\n#     chunker = DocumentChunker()\n    \n#     # Process single document\n#     # chunks = chunker.process_document(\"document.docx\", \"output_chunks.json\")\n    \n#     # Process multiple documents\n#     # chunker.batch_process(\"input_documents/\", \"output_chunks/\")\n    \n#     # Example with the provided documents\n#     sample_text = '''\n#     **Organization Background**\n#     Please provide a brief description of your organization's mission and history.\n    \n#     Cambio Labs was established in 2021 by SebastiÃ¡n MartÃ­n, a social entrepreneur and educator, \n#     in response to educational and employment inequities faced by low-income BIPOC youth.\n    \n#     **Program Description**\n#     Describe the program, its purpose and how it will be implemented.\n    \n#     Cambio Energy is an initiative to create access to clean energy jobs, utilities savings \n#     through community solar projects, and green entrepreneurship accelerators.\n#     '''\n    \n#     # Save sample text to file for testing\n#     with open('sample_doc.txt', 'w') as f:\n#         f.write(sample_text)\n    \n#     # Process the sample\n#     chunks = chunker.process_document('sample_doc.txt', 'sample_output.json')\n    \n#     # Print results\n#     for chunk in chunks:\n#         print(f\"\\nChunk {chunk.chunk_id}: {chunk.header}\")\n#         print(f\"Topics: {chunk.topics}\")\n#         print(f\"Word count: {chunk.word_count}\")\n#         print(f\"Confidence: {chunk.confidence_score:.2f}\")\n\n# if __name__ == \"__main__\":\n#     main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:14:18.289587Z","iopub.execute_input":"2025-07-24T15:14:18.289878Z","iopub.status.idle":"2025-07-24T15:14:18.327119Z","shell.execute_reply.started":"2025-07-24T15:14:18.289857Z","shell.execute_reply":"2025-07-24T15:14:18.326435Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# === FILE CHECKER ===\ndef check_for_new_files():\n    print(f\"\\U0001F50D Checking for new files at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    chunker = DocumentChunker()\n    file_list = drive.ListFile({'q': f\"'{WATCH_FOLDER_ID}' in parents and trashed=false\"}).GetList()\n\n    for file in file_list:\n        title = file['title']\n        file_id = file['id']\n        modified_time = file['modifiedDate']\n        sanitized_title = sanitize_filename(title)\n        file_path = DOWNLOAD_PATH / f\"{sanitized_title}.docx\"\n\n        # existing = structured_col.find_one({\"metadata.title\": title})\n        # if existing and existing.get(\"metadata\", {}).get(\"modifiedDate\") == modified_time:\n        #     print(f\"âœ… Already processed: {title}\")\n        #     continue\n\n        print(f\"â¬‡ Downloading: {title}\")\n        file.GetContentFile(str(file_path), mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document')\n\n        loader = UnstructuredFileLoader(str(file_path))\n        pages = loader.load()\n        raw_text = pages[0].page_content\n        doc_id = sanitized_title.replace(\" \", \"_\").lower()\n\n        #USING CHUNKER\n        try:\n            print(f\"Processing Chunks for : {title}\")\n            chunks_output_path = Path(\"/kaggle/working\") / f\"{sanitized_title}_chunks.json\"\n            chunks= chunker.process_document(str(file_path),str(chunks_output_path))\n            print(f\"âœ… Successfully created {len(chunks)} chunks\")\n            for i, chunk in enumerate(chunks[:3]):\n                print(f\"Chunk{chunk.chunk_id}:{chunk.header}\")\n                print(f\"    Topics: {chunk.topics}\")\n                print(f\"    Word count: {chunk.word_count}\")\n                print(f\"    Confidence: {chunk.confidence_score:.2f}\")\n                print()\n            if len(chunks) > 3:\n                print(f\"  ... and {len(chunks) - 3} more chunks\")\n        except Exception as e:\n            print(f\"âŒ Error processing chunks for {title}: {e}\")\n        # qa_pairs = extract_qa_pairs(raw_text)\n\n        # doc_entry = {\n        #     \"doc_id\": doc_id,\n        #     \"metadata\": {\n        #         \"title\": title,\n        #         \"doc_id\": doc_id,\n        #         \"file_id\": file_id,\n        #         \"modifiedDate\": modified_time,\n        #         \"word_count\": len(raw_text.split()),\n        #     },\n        #     \"chunks\": []\n        # }\n\n        # flat_chunks = []\n\n        # for i, pair in enumerate(qa_pairs):\n        #     q, a = pair['question'], pair['answer']\n        #     categories = categorize_chunk(a, q)\n        #     if categories == [\"Miscellaneous\"]:\n        #         annotations = annotate_chunk(a)\n        #         categories = [annotations[\"category\"]] if annotations[\"category\"] else [\"Miscellaneous\"]\n        #     for cat in categories:\n        #         chunk_id = f\"{doc_id}_chunk_{i}_{cat.replace(' ', '_')}\"\n        #         try:\n        #             embedding = embedding_model.encode(a).tolist()\n        #         except Exception as e:\n        #             print(f\"âš ï¸ Embedding error: {e}\")\n        #             embedding = None\n\n        #         chunk_data = {\n        #             \"chunk_id\": chunk_id,\n        #             \"text\": a,\n        #             \"embedding\": embedding,\n        #             \"question\": q,\n        #             \"answer\": a,\n        #             \"word_count\": len(a.split()),\n        #             \"metadata\": {\n        #                 \"doc_id\": doc_id,\n        #                 \"title\": title,\n        #                 \"file_id\": file_id,\n        #                 \"modifiedDate\": modified_time,\n        #                 \"category\": cat\n        #             }\n        #         }\n        #         doc_entry[\"chunks\"].append(chunk_data)\n        #         flat_chunks.append(chunk_data)\n\n        # structured_col.delete_many({\"metadata.title\": title})\n        # flat_col.delete_many({\"metadata.doc_id\": doc_id})\n\n        # structured_col.insert_one(doc_entry)\n        # if flat_chunks:\n        #     flat_col.insert_many(flat_chunks)\n        # with open(f\"/kaggle/working/{doc_id}_structured.json\", \"w\") as f:\n        #     json.dump(doc_entry, f, indent=2)\n        # with open(f\"/kaggle/working/{doc_id}_flat.json\", \"w\") as f:\n        #     json.dump(flat_chunks, f, indent=2)\n\n\n        # print(f\"âœ… Processed and inserted: {title}\")\n\n        # try:\n        #     os.remove(file_path)\n        # except Exception as e:\n        #     print(f\"âš ï¸ Failed to delete file: {e}\")\n\n    print(\"âœ… Check complete.\\n\")\n\nif __name__ == \"__main__\":\n    check_for_new_files()\n# === LOOP ===\n# while True:\n#     check_for_new_files()\n#     print(\"â²ï¸ Sleeping for 5 minutes...\\n\")\n#     time.sleep(300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:14:50.139461Z","iopub.execute_input":"2025-07-24T15:14:50.139754Z","iopub.status.idle":"2025-07-24T15:14:54.143991Z","shell.execute_reply.started":"2025-07-24T15:14:50.139736Z","shell.execute_reply":"2025-07-24T15:14:54.143226Z"}},"outputs":[{"name":"stdout","text":"ğŸ” Checking for new files at 2025-07-24 15:14:50\nâ¬‡ Downloading: Copy of DATA - 2024 Con Edison - Focus Grant Application\nProcessing Chunks for : Copy of DATA - 2024 Con Edison - Focus Grant Application\nâœ… Successfully created 19 chunks\nChunk1:Organization Background\n    Topics: []\n    Word count: 11\n    Confidence: 0.40\n\nChunk2:Guidance: Please limit your response to approximately 300 words.\n    Topics: ['education', 'entrepreneurship', 'solar']\n    Word count: 206\n    Confidence: 0.80\n\nChunk3:Program Description\n    Topics: ['funding', 'program']\n    Word count: 29\n    Confidence: 0.70\n\n  ... and 16 more chunks\nâ¬‡ Downloading: Copy of DATA - VELA 2022 - May, Microgrant\nProcessing Chunks for : Copy of DATA - VELA 2022 - May, Microgrant\nâœ… Successfully created 13 chunks\nChunk1:What is the name of your idea?\n    Topics: ['education', 'entrepreneurship']\n    Word count: 8\n    Confidence: 0.40\n\nChunk2:Please describe the idea. What is the idea trying to do, and for whom?\n    Topics: ['education', 'entrepreneurship', 'workforce']\n    Word count: 150\n    Confidence: 0.80\n\nChunk3:What is the origin story for the program that VELA would be supporting? How did the program come to be?\n    Topics: ['education', 'entrepreneurship', 'workforce']\n    Word count: 97\n    Confidence: 0.70\n\n  ... and 10 more chunks\nâ¬‡ Downloading: Copy of DATA - Queens Tech Challenge\nProcessing Chunks for : Copy of DATA - Queens Tech Challenge\nâœ… Successfully created 16 chunks\nChunk1:Short description of your company/business. 300 Characters\n    Topics: ['education', 'entrepreneurship', 'technology']\n    Word count: 26\n    Confidence: 0.70\n\nChunk2:How much revenue does your business have in the last 12 months?\n    Topics: ['entrepreneurship', 'funding']\n    Word count: 1\n    Confidence: 0.40\n\nChunk3:If you don't have paid customers yet, how have you tested the product/service with your target market so far?\n    Topics: ['education', 'entrepreneurship', 'solar']\n    Word count: 181\n    Confidence: 0.80\n\n  ... and 13 more chunks\nâ¬‡ Downloading: Copy of DATA - Application YASS\nProcessing Chunks for : Copy of DATA - Application YASS\nâœ… Successfully created 27 chunks\nChunk1:How many students did you serve or reach in total in 2022?\n    Topics: ['education']\n    Word count: 1\n    Confidence: 0.40\n\nChunk2:How many total students are you serving now in 2023?\n    Topics: ['education']\n    Word count: 1\n    Confidence: 0.40\n\nChunk3:How many total students do you plan to serve in 2024?\n    Topics: ['education']\n    Word count: 1\n    Confidence: 0.40\n\n  ... and 24 more chunks\nâœ… Check complete.\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# # === Main ===\n# def check_for_new_files():\n#     print(f\"ğŸ” Checking for new files at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n#     file_list = drive.ListFile({'q': f\"'{WATCH_FOLDER_ID}' in parents and trashed=false\"}).GetList()\n\n#     for file in file_list:\n#         title = file['title']\n#         file_id = file['id']\n#         modified_time = file['modifiedDate']\n\n#         # Make filename safe\n#         doc_id = sanitize_filename(title.replace(\" \", \"_\").lower())\n\n#         # Skip if structured JSON already exists\n#         if Path(f\"/kaggle/working/{doc_id}_structured.json\").exists():\n#             print(f\"âœ… Already processed locally: {title}\")\n#             continue\n\n#         print(f\"â¬‡ Downloading: {title}\")\n#         file_path = DOWNLOAD_PATH / f\"{sanitize_filename(title)}.docx\"\n#         file.GetContentFile(str(file_path), mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document')\n\n#         loader = UnstructuredFileLoader(str(file_path))\n#         pages = loader.load()\n#         raw_text = pages[0].page_content\n#         word_count = len(raw_text.split())\n\n#         flat_chunks = []\n\n#         for i, page in enumerate(pages):\n#             chunks = chunk_text(page.page_content)\n#             for j, chunk in enumerate(chunks):\n#                 qa = generate_qa_pair(chunk)\n#                 try:\n#                     embedding = embedding_model.encode(chunk).tolist()\n#                 except Exception as e:\n#                     print(f\"âš ï¸ Embedding error: {e}\")\n#                     embedding = None\n\n#                 chunk_data = {\n#                     \"chunk_id\": f\"{doc_id}_chunk_{i}_{j}\",\n#                     \"text\": chunk,\n#                     \"embedding\": embedding,\n#                     \"question\": qa[\"question\"],\n#                     \"answer\": qa[\"answer\"],\n#                     \"word_count\": len(chunk.split()),\n#                     \"metadata\": {\n#                         \"doc_id\": doc_id,\n#                         \"title\": title,\n#                         \"file_id\": file_id,\n#                         \"modifiedDate\": modified_time,\n#                         \"category\": qa[\"category\"]\n#                     }\n#                 }\n#                 flat_chunks.append(chunk_data)\n\n#         doc_entry = {\n#             \"doc_id\": doc_id,\n#             \"metadata\": {\n#                 \"title\": title,\n#                 \"doc_id\": doc_id,\n#                 \"file_id\": file_id,\n#                 \"modifiedDate\": modified_time,\n#                 \"word_count\": word_count\n#             },\n#             \"chunks\": flat_chunks\n#         }\n\n#         # Save flat and structured JSONs in /kaggle/working\n#         with open(f\"/kaggle/working/{doc_id}_structured.json\", \"w\") as f:\n#             json.dump(doc_entry, f, indent=2)\n#         with open(f\"/kaggle/working/{doc_id}_flat.json\", \"w\") as f:\n#             json.dump(flat_chunks, f, indent=2)\n\n#         try:\n#             os.remove(file_path)\n#         except Exception as e:\n#             print(f\"âš ï¸ Failed to delete file: {e}\")\n\n#     print(\"âœ… Check complete.\\n\")\n\n# if __name__ == \"__main__\":\n#     check_for_new_files()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:09:40.489853Z","iopub.execute_input":"2025-07-24T15:09:40.490164Z","iopub.status.idle":"2025-07-24T15:09:40.494873Z","shell.execute_reply.started":"2025-07-24T15:09:40.490133Z","shell.execute_reply":"2025-07-24T15:09:40.494294Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nimport shutil\nfrom pathlib import Path\nimport zipfile\n\nWORKING_DIR = Path(\"/kaggle/working\")\n# STRUCTURED_DIR = WORKING_DIR / \"full_rules_structured\"\n# FLAT_DIR = WORKING_DIR / \"full_rules_flat\"\nAll_Files = WORKING_DIR / \"ALL_FILES\"\n\n# Create folders if they don't exist\n# STRUCTURED_DIR.mkdir(exist_ok=True)\n# FLAT_DIR.mkdir(exist_ok=True)\nAll_Files.mkdir(exist_ok=True)\n# Move JSON files into their respective folders\nfor file in WORKING_DIR.glob(\"*.json\"):\n     if file.name.endswith(\"_chunks.json\"):\n        shutil.move(str(file), All_Files / file.name)\n    # if file.name.endswith(\"_structured.json\"):\n    #     shutil.move(str(file), STRUCTURED_DIR / file.name)\n    # elif file.name.endswith(\"_flat.json\"):\n    #     shutil.move(str(file), FLAT_DIR / file.name)\n\n# Function to zip a folder\ndef zip_folder(folder_path: Path, zip_path: Path):\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in folder_path.rglob('*'):\n            zipf.write(file, arcname=file.relative_to(folder_path))\n\n# Zip both folders\n# zip_folder(STRUCTURED_DIR, WORKING_DIR / \"full_rules_structured.zip\")\n# zip_folder(FLAT_DIR, WORKING_DIR / \"full_rules_flat.zip\")\nzip_folder(All_Files, WORKING_DIR / \"ALL_FILES.zip\")\n\nprint(\"Folders zipped to: ALL_FILES.zip\")\n# print(\" - full_rules_structured.zip\")\n# print(\" - full_rules_flat.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:19:34.882571Z","iopub.execute_input":"2025-07-24T15:19:34.883255Z","iopub.status.idle":"2025-07-24T15:19:34.890225Z","shell.execute_reply.started":"2025-07-24T15:19:34.883232Z","shell.execute_reply":"2025-07-24T15:19:34.889392Z"}},"outputs":[{"name":"stdout","text":"Folders zipped to: ALL_FILES.zip\n","output_type":"stream"}],"execution_count":19}]}
